{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1780927228.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1780927228.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])   \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)  \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1098455523.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1145192065.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1145192065.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1145192065.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/1145192065.py:36: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/3211748656.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/3211748656.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "/var/folders/qx/5dmjvwj50ysf44p31147cjt40000gn/T/ipykernel_9451/3211748656.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.330172\n",
      "Epoch 1, loss: 2.324864\n",
      "Epoch 2, loss: 2.321661\n",
      "Epoch 3, loss: 2.319486\n",
      "Epoch 4, loss: 2.315051\n",
      "Epoch 5, loss: 2.312794\n",
      "Epoch 6, loss: 2.311997\n",
      "Epoch 7, loss: 2.311030\n",
      "Epoch 8, loss: 2.307720\n",
      "Epoch 9, loss: 2.307321\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc1927408b0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.171\n",
      "Epoch 0, loss: 2.293504\n",
      "Epoch 1, loss: 2.294481\n",
      "Epoch 2, loss: 2.292677\n",
      "Epoch 3, loss: 2.293434\n",
      "Epoch 4, loss: 2.292737\n",
      "Epoch 5, loss: 2.295767\n",
      "Epoch 6, loss: 2.291686\n",
      "Epoch 7, loss: 2.294721\n",
      "Epoch 8, loss: 2.293940\n",
      "Epoch 9, loss: 2.292684\n",
      "Epoch 10, loss: 2.294951\n",
      "Epoch 11, loss: 2.292133\n",
      "Epoch 12, loss: 2.294112\n",
      "Epoch 13, loss: 2.297670\n",
      "Epoch 14, loss: 2.296757\n",
      "Epoch 15, loss: 2.293543\n",
      "Epoch 16, loss: 2.293842\n",
      "Epoch 17, loss: 2.298310\n",
      "Epoch 18, loss: 2.295565\n",
      "Epoch 19, loss: 2.290984\n",
      "Epoch 20, loss: 2.298251\n",
      "Epoch 21, loss: 2.296783\n",
      "Epoch 22, loss: 2.292371\n",
      "Epoch 23, loss: 2.295656\n",
      "Epoch 24, loss: 2.296400\n",
      "Epoch 25, loss: 2.295967\n",
      "Epoch 26, loss: 2.298829\n",
      "Epoch 27, loss: 2.293550\n",
      "Epoch 28, loss: 2.292130\n",
      "Epoch 29, loss: 2.293090\n",
      "Epoch 30, loss: 2.295027\n",
      "Epoch 31, loss: 2.297270\n",
      "Epoch 32, loss: 2.300594\n",
      "Epoch 33, loss: 2.297313\n",
      "Epoch 34, loss: 2.297115\n",
      "Epoch 35, loss: 2.292840\n",
      "Epoch 36, loss: 2.293308\n",
      "Epoch 37, loss: 2.292161\n",
      "Epoch 38, loss: 2.299124\n",
      "Epoch 39, loss: 2.295028\n",
      "Epoch 40, loss: 2.298774\n",
      "Epoch 41, loss: 2.297909\n",
      "Epoch 42, loss: 2.297770\n",
      "Epoch 43, loss: 2.296606\n",
      "Epoch 44, loss: 2.295926\n",
      "Epoch 45, loss: 2.297988\n",
      "Epoch 46, loss: 2.290916\n",
      "Epoch 47, loss: 2.297969\n",
      "Epoch 48, loss: 2.294839\n",
      "Epoch 49, loss: 2.294337\n",
      "Epoch 50, loss: 2.294127\n",
      "Epoch 51, loss: 2.293699\n",
      "Epoch 52, loss: 2.299630\n",
      "Epoch 53, loss: 2.294685\n",
      "Epoch 54, loss: 2.296393\n",
      "Epoch 55, loss: 2.299174\n",
      "Epoch 56, loss: 2.296830\n",
      "Epoch 57, loss: 2.298876\n",
      "Epoch 58, loss: 2.292191\n",
      "Epoch 59, loss: 2.295413\n",
      "Epoch 60, loss: 2.294774\n",
      "Epoch 61, loss: 2.297585\n",
      "Epoch 62, loss: 2.296317\n",
      "Epoch 63, loss: 2.295872\n",
      "Epoch 64, loss: 2.297769\n",
      "Epoch 65, loss: 2.297858\n",
      "Epoch 66, loss: 2.291155\n",
      "Epoch 67, loss: 2.296191\n",
      "Epoch 68, loss: 2.293658\n",
      "Epoch 69, loss: 2.293973\n",
      "Epoch 70, loss: 2.292468\n",
      "Epoch 71, loss: 2.297618\n",
      "Epoch 72, loss: 2.295567\n",
      "Epoch 73, loss: 2.295314\n",
      "Epoch 74, loss: 2.295998\n",
      "Epoch 75, loss: 2.296713\n",
      "Epoch 76, loss: 2.294418\n",
      "Epoch 77, loss: 2.294957\n",
      "Epoch 78, loss: 2.299390\n",
      "Epoch 79, loss: 2.289291\n",
      "Epoch 80, loss: 2.297370\n",
      "Epoch 81, loss: 2.295217\n",
      "Epoch 82, loss: 2.295945\n",
      "Epoch 83, loss: 2.293646\n",
      "Epoch 84, loss: 2.296519\n",
      "Epoch 85, loss: 2.294953\n",
      "Epoch 86, loss: 2.292975\n",
      "Epoch 87, loss: 2.299893\n",
      "Epoch 88, loss: 2.294704\n",
      "Epoch 89, loss: 2.292792\n",
      "Epoch 90, loss: 2.294709\n",
      "Epoch 91, loss: 2.297230\n",
      "Epoch 92, loss: 2.295530\n",
      "Epoch 93, loss: 2.294870\n",
      "Epoch 94, loss: 2.297874\n",
      "Epoch 95, loss: 2.292267\n",
      "Epoch 96, loss: 2.296982\n",
      "Epoch 97, loss: 2.295447\n",
      "Epoch 98, loss: 2.296661\n",
      "Epoch 99, loss: 2.299312\n",
      "Accuracy after training for 100 epochs:  0.173\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.300408\n",
      "Epoch 1, loss: 2.300929\n",
      "Epoch 2, loss: 2.301147\n",
      "Epoch 3, loss: 2.299314\n",
      "Epoch 4, loss: 2.297430\n",
      "Epoch 5, loss: 2.297710\n",
      "Epoch 6, loss: 2.300063\n",
      "Epoch 7, loss: 2.293306\n",
      "Epoch 8, loss: 2.293689\n",
      "Epoch 9, loss: 2.296143\n",
      "Epoch 10, loss: 2.292259\n",
      "Epoch 11, loss: 2.293148\n",
      "Epoch 12, loss: 2.291685\n",
      "Epoch 13, loss: 2.289433\n",
      "Epoch 14, loss: 2.293681\n",
      "Epoch 15, loss: 2.289214\n",
      "Epoch 16, loss: 2.285024\n",
      "Epoch 17, loss: 2.286956\n",
      "Epoch 18, loss: 2.291519\n",
      "Epoch 19, loss: 2.295362\n",
      "Epoch 20, loss: 2.282206\n",
      "Epoch 21, loss: 2.288724\n",
      "Epoch 22, loss: 2.289236\n",
      "Epoch 23, loss: 2.285455\n",
      "Epoch 24, loss: 2.283105\n",
      "Epoch 25, loss: 2.279143\n",
      "Epoch 26, loss: 2.283452\n",
      "Epoch 27, loss: 2.283222\n",
      "Epoch 28, loss: 2.284938\n",
      "Epoch 29, loss: 2.276974\n",
      "Epoch 30, loss: 2.283940\n",
      "Epoch 31, loss: 2.278264\n",
      "Epoch 32, loss: 2.268812\n",
      "Epoch 33, loss: 2.280884\n",
      "Epoch 34, loss: 2.271438\n",
      "Epoch 35, loss: 2.274798\n",
      "Epoch 36, loss: 2.266283\n",
      "Epoch 37, loss: 2.271401\n",
      "Epoch 38, loss: 2.273089\n",
      "Epoch 39, loss: 2.269406\n",
      "Epoch 40, loss: 2.274111\n",
      "Epoch 41, loss: 2.273271\n",
      "Epoch 42, loss: 2.271485\n",
      "Epoch 43, loss: 2.276314\n",
      "Epoch 44, loss: 2.266384\n",
      "Epoch 45, loss: 2.263481\n",
      "Epoch 46, loss: 2.269360\n",
      "Epoch 47, loss: 2.254006\n",
      "Epoch 48, loss: 2.271057\n",
      "Epoch 49, loss: 2.273328\n",
      "Epoch 50, loss: 2.264278\n",
      "Epoch 51, loss: 2.262201\n",
      "Epoch 52, loss: 2.258079\n",
      "Epoch 53, loss: 2.263908\n",
      "Epoch 54, loss: 2.259834\n",
      "Epoch 55, loss: 2.252935\n",
      "Epoch 56, loss: 2.267954\n",
      "Epoch 57, loss: 2.252831\n",
      "Epoch 58, loss: 2.260989\n",
      "Epoch 59, loss: 2.251622\n",
      "Epoch 60, loss: 2.263384\n",
      "Epoch 61, loss: 2.250582\n",
      "Epoch 62, loss: 2.268970\n",
      "Epoch 63, loss: 2.252198\n",
      "Epoch 64, loss: 2.249101\n",
      "Epoch 65, loss: 2.243338\n",
      "Epoch 66, loss: 2.251215\n",
      "Epoch 67, loss: 2.256592\n",
      "Epoch 68, loss: 2.259951\n",
      "Epoch 69, loss: 2.262580\n",
      "Epoch 70, loss: 2.246867\n",
      "Epoch 71, loss: 2.252150\n",
      "Epoch 72, loss: 2.260080\n",
      "Epoch 73, loss: 2.263883\n",
      "Epoch 74, loss: 2.265212\n",
      "Epoch 75, loss: 2.273175\n",
      "Epoch 76, loss: 2.250656\n",
      "Epoch 77, loss: 2.251237\n",
      "Epoch 78, loss: 2.253351\n",
      "Epoch 79, loss: 2.250015\n",
      "Epoch 80, loss: 2.246514\n",
      "Epoch 81, loss: 2.242834\n",
      "Epoch 82, loss: 2.256902\n",
      "Epoch 83, loss: 2.249935\n",
      "Epoch 84, loss: 2.254618\n",
      "Epoch 85, loss: 2.240603\n",
      "Epoch 86, loss: 2.226796\n",
      "Epoch 87, loss: 2.242757\n",
      "Epoch 88, loss: 2.254196\n",
      "Epoch 89, loss: 2.236538\n",
      "Epoch 90, loss: 2.234233\n",
      "Epoch 91, loss: 2.253281\n",
      "Epoch 92, loss: 2.236712\n",
      "Epoch 93, loss: 2.236751\n",
      "Epoch 94, loss: 2.230130\n",
      "Epoch 95, loss: 2.242823\n",
      "Epoch 96, loss: 2.242269\n",
      "Epoch 97, loss: 2.235329\n",
      "Epoch 98, loss: 2.246689\n",
      "Epoch 99, loss: 2.243485\n",
      "Epoch 100, loss: 2.232075\n",
      "Epoch 101, loss: 2.240456\n",
      "Epoch 102, loss: 2.232929\n",
      "Epoch 103, loss: 2.226862\n",
      "Epoch 104, loss: 2.235142\n",
      "Epoch 105, loss: 2.234511\n",
      "Epoch 106, loss: 2.228712\n",
      "Epoch 107, loss: 2.240672\n",
      "Epoch 108, loss: 2.232143\n",
      "Epoch 109, loss: 2.232155\n",
      "Epoch 110, loss: 2.234136\n",
      "Epoch 111, loss: 2.255427\n",
      "Epoch 112, loss: 2.216870\n",
      "Epoch 113, loss: 2.220403\n",
      "Epoch 114, loss: 2.227173\n",
      "Epoch 115, loss: 2.242368\n",
      "Epoch 116, loss: 2.238059\n",
      "Epoch 117, loss: 2.221459\n",
      "Epoch 118, loss: 2.219134\n",
      "Epoch 119, loss: 2.237746\n",
      "Epoch 120, loss: 2.220301\n",
      "Epoch 121, loss: 2.232124\n",
      "Epoch 122, loss: 2.234020\n",
      "Epoch 123, loss: 2.222222\n",
      "Epoch 124, loss: 2.223915\n",
      "Epoch 125, loss: 2.211574\n",
      "Epoch 126, loss: 2.227427\n",
      "Epoch 127, loss: 2.213382\n",
      "Epoch 128, loss: 2.222754\n",
      "Epoch 129, loss: 2.224016\n",
      "Epoch 130, loss: 2.246156\n",
      "Epoch 131, loss: 2.225155\n",
      "Epoch 132, loss: 2.231514\n",
      "Epoch 133, loss: 2.223734\n",
      "Epoch 134, loss: 2.209756\n",
      "Epoch 135, loss: 2.226164\n",
      "Epoch 136, loss: 2.232149\n",
      "Epoch 137, loss: 2.218100\n",
      "Epoch 138, loss: 2.229375\n",
      "Epoch 139, loss: 2.241252\n",
      "Epoch 140, loss: 2.226800\n",
      "Epoch 141, loss: 2.204185\n",
      "Epoch 142, loss: 2.238183\n",
      "Epoch 143, loss: 2.227289\n",
      "Epoch 144, loss: 2.214497\n",
      "Epoch 145, loss: 2.215706\n",
      "Epoch 146, loss: 2.205776\n",
      "Epoch 147, loss: 2.227537\n",
      "Epoch 148, loss: 2.192924\n",
      "Epoch 149, loss: 2.213543\n",
      "Epoch 150, loss: 2.240208\n",
      "Epoch 151, loss: 2.207923\n",
      "Epoch 152, loss: 2.200891\n",
      "Epoch 153, loss: 2.234643\n",
      "Epoch 154, loss: 2.222858\n",
      "Epoch 155, loss: 2.206711\n",
      "Epoch 156, loss: 2.197810\n",
      "Epoch 157, loss: 2.219268\n",
      "Epoch 158, loss: 2.247229\n",
      "Epoch 159, loss: 2.200006\n",
      "Epoch 160, loss: 2.200012\n",
      "Epoch 161, loss: 2.229635\n",
      "Epoch 162, loss: 2.213645\n",
      "Epoch 163, loss: 2.238356\n",
      "Epoch 164, loss: 2.220164\n",
      "Epoch 165, loss: 2.205544\n",
      "Epoch 166, loss: 2.195676\n",
      "Epoch 167, loss: 2.196675\n",
      "Epoch 168, loss: 2.204727\n",
      "Epoch 169, loss: 2.199240\n",
      "Epoch 170, loss: 2.213447\n",
      "Epoch 171, loss: 2.217203\n",
      "Epoch 172, loss: 2.208272\n",
      "Epoch 173, loss: 2.230065\n",
      "Epoch 174, loss: 2.212011\n",
      "Epoch 175, loss: 2.194248\n",
      "Epoch 176, loss: 2.206457\n",
      "Epoch 177, loss: 2.208236\n",
      "Epoch 178, loss: 2.166059\n",
      "Epoch 179, loss: 2.203206\n",
      "Epoch 180, loss: 2.198754\n",
      "Epoch 181, loss: 2.181847\n",
      "Epoch 182, loss: 2.193496\n",
      "Epoch 183, loss: 2.192784\n",
      "Epoch 184, loss: 2.233283\n",
      "Epoch 185, loss: 2.182877\n",
      "Epoch 186, loss: 2.197711\n",
      "Epoch 187, loss: 2.192278\n",
      "Epoch 188, loss: 2.237912\n",
      "Epoch 189, loss: 2.185756\n",
      "Epoch 190, loss: 2.218154\n",
      "Epoch 191, loss: 2.197274\n",
      "Epoch 192, loss: 2.205923\n",
      "Epoch 193, loss: 2.201074\n",
      "Epoch 194, loss: 2.210746\n",
      "Epoch 195, loss: 2.222435\n",
      "Epoch 196, loss: 2.205872\n",
      "Epoch 197, loss: 2.189550\n",
      "Epoch 198, loss: 2.205326\n",
      "Epoch 199, loss: 2.188024\n",
      "Epoch 0, loss: 2.301935\n",
      "Epoch 1, loss: 2.301675\n",
      "Epoch 2, loss: 2.299130\n",
      "Epoch 3, loss: 2.299545\n",
      "Epoch 4, loss: 2.297184\n",
      "Epoch 5, loss: 2.295796\n",
      "Epoch 6, loss: 2.296552\n",
      "Epoch 7, loss: 2.295935\n",
      "Epoch 8, loss: 2.292777\n",
      "Epoch 9, loss: 2.294002\n",
      "Epoch 10, loss: 2.292860\n",
      "Epoch 11, loss: 2.292843\n",
      "Epoch 12, loss: 2.296232\n",
      "Epoch 13, loss: 2.293833\n",
      "Epoch 14, loss: 2.293111\n",
      "Epoch 15, loss: 2.288025\n",
      "Epoch 16, loss: 2.286548\n",
      "Epoch 17, loss: 2.287317\n",
      "Epoch 18, loss: 2.286048\n",
      "Epoch 19, loss: 2.281571\n",
      "Epoch 20, loss: 2.283506\n",
      "Epoch 21, loss: 2.286803\n",
      "Epoch 22, loss: 2.285324\n",
      "Epoch 23, loss: 2.280935\n",
      "Epoch 24, loss: 2.284262\n",
      "Epoch 25, loss: 2.283273\n",
      "Epoch 26, loss: 2.281134\n",
      "Epoch 27, loss: 2.275007\n",
      "Epoch 28, loss: 2.278457\n",
      "Epoch 29, loss: 2.276517\n",
      "Epoch 30, loss: 2.277152\n",
      "Epoch 31, loss: 2.278918\n",
      "Epoch 32, loss: 2.271536\n",
      "Epoch 33, loss: 2.280018\n",
      "Epoch 34, loss: 2.272761\n",
      "Epoch 35, loss: 2.283808\n",
      "Epoch 36, loss: 2.270332\n",
      "Epoch 37, loss: 2.270092\n",
      "Epoch 38, loss: 2.276666\n",
      "Epoch 39, loss: 2.274701\n",
      "Epoch 40, loss: 2.266217\n",
      "Epoch 41, loss: 2.266657\n",
      "Epoch 42, loss: 2.269656\n",
      "Epoch 43, loss: 2.267971\n",
      "Epoch 44, loss: 2.262891\n",
      "Epoch 45, loss: 2.263813\n",
      "Epoch 46, loss: 2.264647\n",
      "Epoch 47, loss: 2.265011\n",
      "Epoch 48, loss: 2.267296\n",
      "Epoch 49, loss: 2.257126\n",
      "Epoch 50, loss: 2.275974\n",
      "Epoch 51, loss: 2.261082\n",
      "Epoch 52, loss: 2.256174\n",
      "Epoch 53, loss: 2.250797\n",
      "Epoch 54, loss: 2.266632\n",
      "Epoch 55, loss: 2.257976\n",
      "Epoch 56, loss: 2.269038\n",
      "Epoch 57, loss: 2.257082\n",
      "Epoch 58, loss: 2.264285\n",
      "Epoch 59, loss: 2.256545\n",
      "Epoch 60, loss: 2.271458\n",
      "Epoch 61, loss: 2.258603\n",
      "Epoch 62, loss: 2.256499\n",
      "Epoch 63, loss: 2.247468\n",
      "Epoch 64, loss: 2.249930\n",
      "Epoch 65, loss: 2.257058\n",
      "Epoch 66, loss: 2.254281\n",
      "Epoch 67, loss: 2.240204\n",
      "Epoch 68, loss: 2.271354\n",
      "Epoch 69, loss: 2.260441\n",
      "Epoch 70, loss: 2.242520\n",
      "Epoch 71, loss: 2.256610\n",
      "Epoch 72, loss: 2.246383\n",
      "Epoch 73, loss: 2.243431\n",
      "Epoch 74, loss: 2.263146\n",
      "Epoch 75, loss: 2.248882\n",
      "Epoch 76, loss: 2.247486\n",
      "Epoch 77, loss: 2.249017\n",
      "Epoch 78, loss: 2.253777\n",
      "Epoch 79, loss: 2.260363\n",
      "Epoch 80, loss: 2.249624\n",
      "Epoch 81, loss: 2.246769\n",
      "Epoch 82, loss: 2.239101\n",
      "Epoch 83, loss: 2.229514\n",
      "Epoch 84, loss: 2.263762\n",
      "Epoch 85, loss: 2.243603\n",
      "Epoch 86, loss: 2.241028\n",
      "Epoch 87, loss: 2.248556\n",
      "Epoch 88, loss: 2.218880\n",
      "Epoch 89, loss: 2.249497\n",
      "Epoch 90, loss: 2.247356\n",
      "Epoch 91, loss: 2.240608\n",
      "Epoch 92, loss: 2.226832\n",
      "Epoch 93, loss: 2.238069\n",
      "Epoch 94, loss: 2.247459\n",
      "Epoch 95, loss: 2.235492\n",
      "Epoch 96, loss: 2.240240\n",
      "Epoch 97, loss: 2.235632\n",
      "Epoch 98, loss: 2.252319\n",
      "Epoch 99, loss: 2.228284\n",
      "Epoch 100, loss: 2.233372\n",
      "Epoch 101, loss: 2.246558\n",
      "Epoch 102, loss: 2.248686\n",
      "Epoch 103, loss: 2.213968\n",
      "Epoch 104, loss: 2.236390\n",
      "Epoch 105, loss: 2.244907\n",
      "Epoch 106, loss: 2.230331\n",
      "Epoch 107, loss: 2.238085\n",
      "Epoch 108, loss: 2.217244\n",
      "Epoch 109, loss: 2.230351\n",
      "Epoch 110, loss: 2.233364\n",
      "Epoch 111, loss: 2.227328\n",
      "Epoch 112, loss: 2.231848\n",
      "Epoch 113, loss: 2.221861\n",
      "Epoch 114, loss: 2.228026\n",
      "Epoch 115, loss: 2.214552\n",
      "Epoch 116, loss: 2.219514\n",
      "Epoch 117, loss: 2.241341\n",
      "Epoch 118, loss: 2.227462\n",
      "Epoch 119, loss: 2.221635\n",
      "Epoch 120, loss: 2.211068\n",
      "Epoch 121, loss: 2.245933\n",
      "Epoch 122, loss: 2.218889\n",
      "Epoch 123, loss: 2.240386\n",
      "Epoch 124, loss: 2.199160\n",
      "Epoch 125, loss: 2.222436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126, loss: 2.216537\n",
      "Epoch 127, loss: 2.213265\n",
      "Epoch 128, loss: 2.216566\n",
      "Epoch 129, loss: 2.195551\n",
      "Epoch 130, loss: 2.247987\n",
      "Epoch 131, loss: 2.215070\n",
      "Epoch 132, loss: 2.233293\n",
      "Epoch 133, loss: 2.208474\n",
      "Epoch 134, loss: 2.218856\n",
      "Epoch 135, loss: 2.232945\n",
      "Epoch 136, loss: 2.224680\n",
      "Epoch 137, loss: 2.214830\n",
      "Epoch 138, loss: 2.204754\n",
      "Epoch 139, loss: 2.208081\n",
      "Epoch 140, loss: 2.229585\n",
      "Epoch 141, loss: 2.218196\n",
      "Epoch 142, loss: 2.207449\n",
      "Epoch 143, loss: 2.207832\n",
      "Epoch 144, loss: 2.220846\n",
      "Epoch 145, loss: 2.218441\n",
      "Epoch 146, loss: 2.232677\n",
      "Epoch 147, loss: 2.215297\n",
      "Epoch 148, loss: 2.227589\n",
      "Epoch 149, loss: 2.203196\n",
      "Epoch 150, loss: 2.217012\n",
      "Epoch 151, loss: 2.235460\n",
      "Epoch 152, loss: 2.205487\n",
      "Epoch 153, loss: 2.207417\n",
      "Epoch 154, loss: 2.234079\n",
      "Epoch 155, loss: 2.211488\n",
      "Epoch 156, loss: 2.192524\n",
      "Epoch 157, loss: 2.243479\n",
      "Epoch 158, loss: 2.223765\n",
      "Epoch 159, loss: 2.216923\n",
      "Epoch 160, loss: 2.185164\n",
      "Epoch 161, loss: 2.216274\n",
      "Epoch 162, loss: 2.231226\n",
      "Epoch 163, loss: 2.204963\n",
      "Epoch 164, loss: 2.222798\n",
      "Epoch 165, loss: 2.228050\n",
      "Epoch 166, loss: 2.208354\n",
      "Epoch 167, loss: 2.185910\n",
      "Epoch 168, loss: 2.223157\n",
      "Epoch 169, loss: 2.201376\n",
      "Epoch 170, loss: 2.207219\n",
      "Epoch 171, loss: 2.214149\n",
      "Epoch 172, loss: 2.190744\n",
      "Epoch 173, loss: 2.219939\n",
      "Epoch 174, loss: 2.207728\n",
      "Epoch 175, loss: 2.185021\n",
      "Epoch 176, loss: 2.200918\n",
      "Epoch 177, loss: 2.194508\n",
      "Epoch 178, loss: 2.205860\n",
      "Epoch 179, loss: 2.213328\n",
      "Epoch 180, loss: 2.203174\n",
      "Epoch 181, loss: 2.218258\n",
      "Epoch 182, loss: 2.193693\n",
      "Epoch 183, loss: 2.209699\n",
      "Epoch 184, loss: 2.187561\n",
      "Epoch 185, loss: 2.228773\n",
      "Epoch 186, loss: 2.195686\n",
      "Epoch 187, loss: 2.186827\n",
      "Epoch 188, loss: 2.226544\n",
      "Epoch 189, loss: 2.195571\n",
      "Epoch 190, loss: 2.216420\n",
      "Epoch 191, loss: 2.222741\n",
      "Epoch 192, loss: 2.195324\n",
      "Epoch 193, loss: 2.220526\n",
      "Epoch 194, loss: 2.189323\n",
      "Epoch 195, loss: 2.196622\n",
      "Epoch 196, loss: 2.229813\n",
      "Epoch 197, loss: 2.198726\n",
      "Epoch 198, loss: 2.179227\n",
      "Epoch 199, loss: 2.207219\n",
      "Epoch 0, loss: 2.301794\n",
      "Epoch 1, loss: 2.299489\n",
      "Epoch 2, loss: 2.301699\n",
      "Epoch 3, loss: 2.298085\n",
      "Epoch 4, loss: 2.297159\n",
      "Epoch 5, loss: 2.295105\n",
      "Epoch 6, loss: 2.297029\n",
      "Epoch 7, loss: 2.294590\n",
      "Epoch 8, loss: 2.295078\n",
      "Epoch 9, loss: 2.296048\n",
      "Epoch 10, loss: 2.290651\n",
      "Epoch 11, loss: 2.293827\n",
      "Epoch 12, loss: 2.296187\n",
      "Epoch 13, loss: 2.289773\n",
      "Epoch 14, loss: 2.288508\n",
      "Epoch 15, loss: 2.281705\n",
      "Epoch 16, loss: 2.290245\n",
      "Epoch 17, loss: 2.288985\n",
      "Epoch 18, loss: 2.286562\n",
      "Epoch 19, loss: 2.285074\n",
      "Epoch 20, loss: 2.277903\n",
      "Epoch 21, loss: 2.282600\n",
      "Epoch 22, loss: 2.284453\n",
      "Epoch 23, loss: 2.282147\n",
      "Epoch 24, loss: 2.288623\n",
      "Epoch 25, loss: 2.281564\n",
      "Epoch 26, loss: 2.280702\n",
      "Epoch 27, loss: 2.281166\n",
      "Epoch 28, loss: 2.281557\n",
      "Epoch 29, loss: 2.275075\n",
      "Epoch 30, loss: 2.279781\n",
      "Epoch 31, loss: 2.273134\n",
      "Epoch 32, loss: 2.280185\n",
      "Epoch 33, loss: 2.278648\n",
      "Epoch 34, loss: 2.262225\n",
      "Epoch 35, loss: 2.270539\n",
      "Epoch 36, loss: 2.267162\n",
      "Epoch 37, loss: 2.268569\n",
      "Epoch 38, loss: 2.274442\n",
      "Epoch 39, loss: 2.261733\n",
      "Epoch 40, loss: 2.276542\n",
      "Epoch 41, loss: 2.264942\n",
      "Epoch 42, loss: 2.274040\n",
      "Epoch 43, loss: 2.264757\n",
      "Epoch 44, loss: 2.258811\n",
      "Epoch 45, loss: 2.256306\n",
      "Epoch 46, loss: 2.260524\n",
      "Epoch 47, loss: 2.258978\n",
      "Epoch 48, loss: 2.264082\n",
      "Epoch 49, loss: 2.260260\n",
      "Epoch 50, loss: 2.268819\n",
      "Epoch 51, loss: 2.263451\n",
      "Epoch 52, loss: 2.268620\n",
      "Epoch 53, loss: 2.263709\n",
      "Epoch 54, loss: 2.247183\n",
      "Epoch 55, loss: 2.266655\n",
      "Epoch 56, loss: 2.249204\n",
      "Epoch 57, loss: 2.268022\n",
      "Epoch 58, loss: 2.261790\n",
      "Epoch 59, loss: 2.256101\n",
      "Epoch 60, loss: 2.264807\n",
      "Epoch 61, loss: 2.253691\n",
      "Epoch 62, loss: 2.251124\n",
      "Epoch 63, loss: 2.273639\n",
      "Epoch 64, loss: 2.259077\n",
      "Epoch 65, loss: 2.253245\n",
      "Epoch 66, loss: 2.259764\n",
      "Epoch 67, loss: 2.239313\n",
      "Epoch 68, loss: 2.245716\n",
      "Epoch 69, loss: 2.254859\n",
      "Epoch 70, loss: 2.252792\n",
      "Epoch 71, loss: 2.246662\n",
      "Epoch 72, loss: 2.254059\n",
      "Epoch 73, loss: 2.244331\n",
      "Epoch 74, loss: 2.246704\n",
      "Epoch 75, loss: 2.244804\n",
      "Epoch 76, loss: 2.250936\n",
      "Epoch 77, loss: 2.250363\n",
      "Epoch 78, loss: 2.240302\n",
      "Epoch 79, loss: 2.250511\n",
      "Epoch 80, loss: 2.253087\n",
      "Epoch 81, loss: 2.258876\n",
      "Epoch 82, loss: 2.242386\n",
      "Epoch 83, loss: 2.251153\n",
      "Epoch 84, loss: 2.243697\n",
      "Epoch 85, loss: 2.247528\n",
      "Epoch 86, loss: 2.244854\n",
      "Epoch 87, loss: 2.244793\n",
      "Epoch 88, loss: 2.242610\n",
      "Epoch 89, loss: 2.259839\n",
      "Epoch 90, loss: 2.251849\n",
      "Epoch 91, loss: 2.237052\n",
      "Epoch 92, loss: 2.239302\n",
      "Epoch 93, loss: 2.237451\n",
      "Epoch 94, loss: 2.238638\n",
      "Epoch 95, loss: 2.223716\n",
      "Epoch 96, loss: 2.242152\n",
      "Epoch 97, loss: 2.244127\n",
      "Epoch 98, loss: 2.249365\n",
      "Epoch 99, loss: 2.234429\n",
      "Epoch 100, loss: 2.236763\n",
      "Epoch 101, loss: 2.225159\n",
      "Epoch 102, loss: 2.229259\n",
      "Epoch 103, loss: 2.229170\n",
      "Epoch 104, loss: 2.220065\n",
      "Epoch 105, loss: 2.216161\n",
      "Epoch 106, loss: 2.252472\n",
      "Epoch 107, loss: 2.235128\n",
      "Epoch 108, loss: 2.227307\n",
      "Epoch 109, loss: 2.251892\n",
      "Epoch 110, loss: 2.227627\n",
      "Epoch 111, loss: 2.220364\n",
      "Epoch 112, loss: 2.232572\n",
      "Epoch 113, loss: 2.239977\n",
      "Epoch 114, loss: 2.234678\n",
      "Epoch 115, loss: 2.242651\n",
      "Epoch 116, loss: 2.212124\n",
      "Epoch 117, loss: 2.227865\n",
      "Epoch 118, loss: 2.231326\n",
      "Epoch 119, loss: 2.198570\n",
      "Epoch 120, loss: 2.206732\n",
      "Epoch 121, loss: 2.238288\n",
      "Epoch 122, loss: 2.214512\n",
      "Epoch 123, loss: 2.234737\n",
      "Epoch 124, loss: 2.215274\n",
      "Epoch 125, loss: 2.209508\n",
      "Epoch 126, loss: 2.228353\n",
      "Epoch 127, loss: 2.225631\n",
      "Epoch 128, loss: 2.231494\n",
      "Epoch 129, loss: 2.228399\n",
      "Epoch 130, loss: 2.218087\n",
      "Epoch 131, loss: 2.213733\n",
      "Epoch 132, loss: 2.237223\n",
      "Epoch 133, loss: 2.233243\n",
      "Epoch 134, loss: 2.227088\n",
      "Epoch 135, loss: 2.225459\n",
      "Epoch 136, loss: 2.218633\n",
      "Epoch 137, loss: 2.234824\n",
      "Epoch 138, loss: 2.212455\n",
      "Epoch 139, loss: 2.216048\n",
      "Epoch 140, loss: 2.228826\n",
      "Epoch 141, loss: 2.208006\n",
      "Epoch 142, loss: 2.216146\n",
      "Epoch 143, loss: 2.235877\n",
      "Epoch 144, loss: 2.234939\n",
      "Epoch 145, loss: 2.226776\n",
      "Epoch 146, loss: 2.216363\n",
      "Epoch 147, loss: 2.196264\n",
      "Epoch 148, loss: 2.201915\n",
      "Epoch 149, loss: 2.225035\n",
      "Epoch 150, loss: 2.232218\n",
      "Epoch 151, loss: 2.208284\n",
      "Epoch 152, loss: 2.220799\n",
      "Epoch 153, loss: 2.229383\n",
      "Epoch 154, loss: 2.226384\n",
      "Epoch 155, loss: 2.210816\n",
      "Epoch 156, loss: 2.217449\n",
      "Epoch 157, loss: 2.176030\n",
      "Epoch 158, loss: 2.218272\n",
      "Epoch 159, loss: 2.218610\n",
      "Epoch 160, loss: 2.201569\n",
      "Epoch 161, loss: 2.225057\n",
      "Epoch 162, loss: 2.229045\n",
      "Epoch 163, loss: 2.220041\n",
      "Epoch 164, loss: 2.215771\n",
      "Epoch 165, loss: 2.227247\n",
      "Epoch 166, loss: 2.198499\n",
      "Epoch 167, loss: 2.206966\n",
      "Epoch 168, loss: 2.207742\n",
      "Epoch 169, loss: 2.219581\n",
      "Epoch 170, loss: 2.210603\n",
      "Epoch 171, loss: 2.216027\n",
      "Epoch 172, loss: 2.189859\n",
      "Epoch 173, loss: 2.208650\n",
      "Epoch 174, loss: 2.171561\n",
      "Epoch 175, loss: 2.230614\n",
      "Epoch 176, loss: 2.230699\n",
      "Epoch 177, loss: 2.199064\n",
      "Epoch 178, loss: 2.200221\n",
      "Epoch 179, loss: 2.213269\n",
      "Epoch 180, loss: 2.224969\n",
      "Epoch 181, loss: 2.211719\n",
      "Epoch 182, loss: 2.207192\n",
      "Epoch 183, loss: 2.212315\n",
      "Epoch 184, loss: 2.201248\n",
      "Epoch 185, loss: 2.205555\n",
      "Epoch 186, loss: 2.208551\n",
      "Epoch 187, loss: 2.207720\n",
      "Epoch 188, loss: 2.199712\n",
      "Epoch 189, loss: 2.213597\n",
      "Epoch 190, loss: 2.213608\n",
      "Epoch 191, loss: 2.207958\n",
      "Epoch 192, loss: 2.182449\n",
      "Epoch 193, loss: 2.200766\n",
      "Epoch 194, loss: 2.196144\n",
      "Epoch 195, loss: 2.192225\n",
      "Epoch 196, loss: 2.195036\n",
      "Epoch 197, loss: 2.234741\n",
      "Epoch 198, loss: 2.190858\n",
      "Epoch 199, loss: 2.225090\n",
      "Epoch 0, loss: 2.301990\n",
      "Epoch 1, loss: 2.300341\n",
      "Epoch 2, loss: 2.302294\n",
      "Epoch 3, loss: 2.302368\n",
      "Epoch 4, loss: 2.302706\n",
      "Epoch 5, loss: 2.301809\n",
      "Epoch 6, loss: 2.302526\n",
      "Epoch 7, loss: 2.301716\n",
      "Epoch 8, loss: 2.300609\n",
      "Epoch 9, loss: 2.299575\n",
      "Epoch 10, loss: 2.301639\n",
      "Epoch 11, loss: 2.301241\n",
      "Epoch 12, loss: 2.301875\n",
      "Epoch 13, loss: 2.298928\n",
      "Epoch 14, loss: 2.302214\n",
      "Epoch 15, loss: 2.300887\n",
      "Epoch 16, loss: 2.300396\n",
      "Epoch 17, loss: 2.300390\n",
      "Epoch 18, loss: 2.300395\n",
      "Epoch 19, loss: 2.301098\n",
      "Epoch 20, loss: 2.298242\n",
      "Epoch 21, loss: 2.301040\n",
      "Epoch 22, loss: 2.301694\n",
      "Epoch 23, loss: 2.300689\n",
      "Epoch 24, loss: 2.298041\n",
      "Epoch 25, loss: 2.298779\n",
      "Epoch 26, loss: 2.298754\n",
      "Epoch 27, loss: 2.297639\n",
      "Epoch 28, loss: 2.298633\n",
      "Epoch 29, loss: 2.296516\n",
      "Epoch 30, loss: 2.298485\n",
      "Epoch 31, loss: 2.299275\n",
      "Epoch 32, loss: 2.297344\n",
      "Epoch 33, loss: 2.298134\n",
      "Epoch 34, loss: 2.299046\n",
      "Epoch 35, loss: 2.298580\n",
      "Epoch 36, loss: 2.299354\n",
      "Epoch 37, loss: 2.299055\n",
      "Epoch 38, loss: 2.297766\n",
      "Epoch 39, loss: 2.295257\n",
      "Epoch 40, loss: 2.296939\n",
      "Epoch 41, loss: 2.298492\n",
      "Epoch 42, loss: 2.297972\n",
      "Epoch 43, loss: 2.297993\n",
      "Epoch 44, loss: 2.300186\n",
      "Epoch 45, loss: 2.298963\n",
      "Epoch 46, loss: 2.298951\n",
      "Epoch 47, loss: 2.297782\n",
      "Epoch 48, loss: 2.298153\n",
      "Epoch 49, loss: 2.293096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, loss: 2.296497\n",
      "Epoch 51, loss: 2.294907\n",
      "Epoch 52, loss: 2.297444\n",
      "Epoch 53, loss: 2.299234\n",
      "Epoch 54, loss: 2.298169\n",
      "Epoch 55, loss: 2.295252\n",
      "Epoch 56, loss: 2.296446\n",
      "Epoch 57, loss: 2.297542\n",
      "Epoch 58, loss: 2.295933\n",
      "Epoch 59, loss: 2.298568\n",
      "Epoch 60, loss: 2.297882\n",
      "Epoch 61, loss: 2.298255\n",
      "Epoch 62, loss: 2.297626\n",
      "Epoch 63, loss: 2.297452\n",
      "Epoch 64, loss: 2.295674\n",
      "Epoch 65, loss: 2.297835\n",
      "Epoch 66, loss: 2.298674\n",
      "Epoch 67, loss: 2.299419\n",
      "Epoch 68, loss: 2.297151\n",
      "Epoch 69, loss: 2.296469\n",
      "Epoch 70, loss: 2.295412\n",
      "Epoch 71, loss: 2.296668\n",
      "Epoch 72, loss: 2.296296\n",
      "Epoch 73, loss: 2.292826\n",
      "Epoch 74, loss: 2.292717\n",
      "Epoch 75, loss: 2.292630\n",
      "Epoch 76, loss: 2.296975\n",
      "Epoch 77, loss: 2.293801\n",
      "Epoch 78, loss: 2.294960\n",
      "Epoch 79, loss: 2.296073\n",
      "Epoch 80, loss: 2.295801\n",
      "Epoch 81, loss: 2.293269\n",
      "Epoch 82, loss: 2.295095\n",
      "Epoch 83, loss: 2.296121\n",
      "Epoch 84, loss: 2.293091\n",
      "Epoch 85, loss: 2.293984\n",
      "Epoch 86, loss: 2.296581\n",
      "Epoch 87, loss: 2.290769\n",
      "Epoch 88, loss: 2.291943\n",
      "Epoch 89, loss: 2.297286\n",
      "Epoch 90, loss: 2.295015\n",
      "Epoch 91, loss: 2.292432\n",
      "Epoch 92, loss: 2.291995\n",
      "Epoch 93, loss: 2.292534\n",
      "Epoch 94, loss: 2.291524\n",
      "Epoch 95, loss: 2.291314\n",
      "Epoch 96, loss: 2.291523\n",
      "Epoch 97, loss: 2.296316\n",
      "Epoch 98, loss: 2.295102\n",
      "Epoch 99, loss: 2.295186\n",
      "Epoch 100, loss: 2.295747\n",
      "Epoch 101, loss: 2.290444\n",
      "Epoch 102, loss: 2.291028\n",
      "Epoch 103, loss: 2.292466\n",
      "Epoch 104, loss: 2.290080\n",
      "Epoch 105, loss: 2.295252\n",
      "Epoch 106, loss: 2.289915\n",
      "Epoch 107, loss: 2.294528\n",
      "Epoch 108, loss: 2.296031\n",
      "Epoch 109, loss: 2.294415\n",
      "Epoch 110, loss: 2.289730\n",
      "Epoch 111, loss: 2.293591\n",
      "Epoch 112, loss: 2.291434\n",
      "Epoch 113, loss: 2.290784\n",
      "Epoch 114, loss: 2.291702\n",
      "Epoch 115, loss: 2.289085\n",
      "Epoch 116, loss: 2.290141\n",
      "Epoch 117, loss: 2.291691\n",
      "Epoch 118, loss: 2.291368\n",
      "Epoch 119, loss: 2.291740\n",
      "Epoch 120, loss: 2.294499\n",
      "Epoch 121, loss: 2.291938\n",
      "Epoch 122, loss: 2.289052\n",
      "Epoch 123, loss: 2.291407\n",
      "Epoch 124, loss: 2.291829\n",
      "Epoch 125, loss: 2.294101\n",
      "Epoch 126, loss: 2.293278\n",
      "Epoch 127, loss: 2.292011\n",
      "Epoch 128, loss: 2.288710\n",
      "Epoch 129, loss: 2.289323\n",
      "Epoch 130, loss: 2.287822\n",
      "Epoch 131, loss: 2.288712\n",
      "Epoch 132, loss: 2.285299\n",
      "Epoch 133, loss: 2.291597\n",
      "Epoch 134, loss: 2.290241\n",
      "Epoch 135, loss: 2.289283\n",
      "Epoch 136, loss: 2.291268\n",
      "Epoch 137, loss: 2.289258\n",
      "Epoch 138, loss: 2.290503\n",
      "Epoch 139, loss: 2.292864\n",
      "Epoch 140, loss: 2.288770\n",
      "Epoch 141, loss: 2.288971\n",
      "Epoch 142, loss: 2.288356\n",
      "Epoch 143, loss: 2.289744\n",
      "Epoch 144, loss: 2.289120\n",
      "Epoch 145, loss: 2.285935\n",
      "Epoch 146, loss: 2.292019\n",
      "Epoch 147, loss: 2.290273\n",
      "Epoch 148, loss: 2.290840\n",
      "Epoch 149, loss: 2.290609\n",
      "Epoch 150, loss: 2.285314\n",
      "Epoch 151, loss: 2.285423\n",
      "Epoch 152, loss: 2.292083\n",
      "Epoch 153, loss: 2.285712\n",
      "Epoch 154, loss: 2.291001\n",
      "Epoch 155, loss: 2.293385\n",
      "Epoch 156, loss: 2.290728\n",
      "Epoch 157, loss: 2.286690\n",
      "Epoch 158, loss: 2.288048\n",
      "Epoch 159, loss: 2.287827\n",
      "Epoch 160, loss: 2.289446\n",
      "Epoch 161, loss: 2.291635\n",
      "Epoch 162, loss: 2.283403\n",
      "Epoch 163, loss: 2.291098\n",
      "Epoch 164, loss: 2.287633\n",
      "Epoch 165, loss: 2.287428\n",
      "Epoch 166, loss: 2.288385\n",
      "Epoch 167, loss: 2.286189\n",
      "Epoch 168, loss: 2.286584\n",
      "Epoch 169, loss: 2.283098\n",
      "Epoch 170, loss: 2.290009\n",
      "Epoch 171, loss: 2.280898\n",
      "Epoch 172, loss: 2.286562\n",
      "Epoch 173, loss: 2.282827\n",
      "Epoch 174, loss: 2.290033\n",
      "Epoch 175, loss: 2.286199\n",
      "Epoch 176, loss: 2.289758\n",
      "Epoch 177, loss: 2.289539\n",
      "Epoch 178, loss: 2.284664\n",
      "Epoch 179, loss: 2.277484\n",
      "Epoch 180, loss: 2.289653\n",
      "Epoch 181, loss: 2.290974\n",
      "Epoch 182, loss: 2.284689\n",
      "Epoch 183, loss: 2.284793\n",
      "Epoch 184, loss: 2.281549\n",
      "Epoch 185, loss: 2.282842\n",
      "Epoch 186, loss: 2.285239\n",
      "Epoch 187, loss: 2.289270\n",
      "Epoch 188, loss: 2.288255\n",
      "Epoch 189, loss: 2.282923\n",
      "Epoch 190, loss: 2.286653\n",
      "Epoch 191, loss: 2.285724\n",
      "Epoch 192, loss: 2.284990\n",
      "Epoch 193, loss: 2.281487\n",
      "Epoch 194, loss: 2.283085\n",
      "Epoch 195, loss: 2.286370\n",
      "Epoch 196, loss: 2.280456\n",
      "Epoch 197, loss: 2.284419\n",
      "Epoch 198, loss: 2.282644\n",
      "Epoch 199, loss: 2.286233\n",
      "Epoch 0, loss: 2.302275\n",
      "Epoch 1, loss: 2.302321\n",
      "Epoch 2, loss: 2.302771\n",
      "Epoch 3, loss: 2.302152\n",
      "Epoch 4, loss: 2.302471\n",
      "Epoch 5, loss: 2.302028\n",
      "Epoch 6, loss: 2.301245\n",
      "Epoch 7, loss: 2.302019\n",
      "Epoch 8, loss: 2.300924\n",
      "Epoch 9, loss: 2.301788\n",
      "Epoch 10, loss: 2.302196\n",
      "Epoch 11, loss: 2.301297\n",
      "Epoch 12, loss: 2.300351\n",
      "Epoch 13, loss: 2.302120\n",
      "Epoch 14, loss: 2.300894\n",
      "Epoch 15, loss: 2.301690\n",
      "Epoch 16, loss: 2.300866\n",
      "Epoch 17, loss: 2.301208\n",
      "Epoch 18, loss: 2.300735\n",
      "Epoch 19, loss: 2.300824\n",
      "Epoch 20, loss: 2.300563\n",
      "Epoch 21, loss: 2.300874\n",
      "Epoch 22, loss: 2.299837\n",
      "Epoch 23, loss: 2.300617\n",
      "Epoch 24, loss: 2.300579\n",
      "Epoch 25, loss: 2.298535\n",
      "Epoch 26, loss: 2.298676\n",
      "Epoch 27, loss: 2.298596\n",
      "Epoch 28, loss: 2.299104\n",
      "Epoch 29, loss: 2.298166\n",
      "Epoch 30, loss: 2.299921\n",
      "Epoch 31, loss: 2.300151\n",
      "Epoch 32, loss: 2.297691\n",
      "Epoch 33, loss: 2.298231\n",
      "Epoch 34, loss: 2.299067\n",
      "Epoch 35, loss: 2.299717\n",
      "Epoch 36, loss: 2.297440\n",
      "Epoch 37, loss: 2.297984\n",
      "Epoch 38, loss: 2.299206\n",
      "Epoch 39, loss: 2.299850\n",
      "Epoch 40, loss: 2.299255\n",
      "Epoch 41, loss: 2.300114\n",
      "Epoch 42, loss: 2.298424\n",
      "Epoch 43, loss: 2.298836\n",
      "Epoch 44, loss: 2.297310\n",
      "Epoch 45, loss: 2.296155\n",
      "Epoch 46, loss: 2.299511\n",
      "Epoch 47, loss: 2.298835\n",
      "Epoch 48, loss: 2.296867\n",
      "Epoch 49, loss: 2.297938\n",
      "Epoch 50, loss: 2.295872\n",
      "Epoch 51, loss: 2.296376\n",
      "Epoch 52, loss: 2.296325\n",
      "Epoch 53, loss: 2.298330\n",
      "Epoch 54, loss: 2.298771\n",
      "Epoch 55, loss: 2.299415\n",
      "Epoch 56, loss: 2.298580\n",
      "Epoch 57, loss: 2.299453\n",
      "Epoch 58, loss: 2.296946\n",
      "Epoch 59, loss: 2.295271\n",
      "Epoch 60, loss: 2.300130\n",
      "Epoch 61, loss: 2.296936\n",
      "Epoch 62, loss: 2.295285\n",
      "Epoch 63, loss: 2.296035\n",
      "Epoch 64, loss: 2.296675\n",
      "Epoch 65, loss: 2.295034\n",
      "Epoch 66, loss: 2.293298\n",
      "Epoch 67, loss: 2.297726\n",
      "Epoch 68, loss: 2.296510\n",
      "Epoch 69, loss: 2.294760\n",
      "Epoch 70, loss: 2.293873\n",
      "Epoch 71, loss: 2.294651\n",
      "Epoch 72, loss: 2.295703\n",
      "Epoch 73, loss: 2.294212\n",
      "Epoch 74, loss: 2.293921\n",
      "Epoch 75, loss: 2.294054\n",
      "Epoch 76, loss: 2.293252\n",
      "Epoch 77, loss: 2.293091\n",
      "Epoch 78, loss: 2.294991\n",
      "Epoch 79, loss: 2.295575\n",
      "Epoch 80, loss: 2.293585\n",
      "Epoch 81, loss: 2.297857\n",
      "Epoch 82, loss: 2.293260\n",
      "Epoch 83, loss: 2.296207\n",
      "Epoch 84, loss: 2.293609\n",
      "Epoch 85, loss: 2.295813\n",
      "Epoch 86, loss: 2.297483\n",
      "Epoch 87, loss: 2.292342\n",
      "Epoch 88, loss: 2.294882\n",
      "Epoch 89, loss: 2.293705\n",
      "Epoch 90, loss: 2.293142\n",
      "Epoch 91, loss: 2.294417\n",
      "Epoch 92, loss: 2.295033\n",
      "Epoch 93, loss: 2.298395\n",
      "Epoch 94, loss: 2.296231\n",
      "Epoch 95, loss: 2.290850\n",
      "Epoch 96, loss: 2.290263\n",
      "Epoch 97, loss: 2.292280\n",
      "Epoch 98, loss: 2.294957\n",
      "Epoch 99, loss: 2.289935\n",
      "Epoch 100, loss: 2.290347\n",
      "Epoch 101, loss: 2.290896\n",
      "Epoch 102, loss: 2.291820\n",
      "Epoch 103, loss: 2.292652\n",
      "Epoch 104, loss: 2.290965\n",
      "Epoch 105, loss: 2.289702\n",
      "Epoch 106, loss: 2.291504\n",
      "Epoch 107, loss: 2.290769\n",
      "Epoch 108, loss: 2.291672\n",
      "Epoch 109, loss: 2.288489\n",
      "Epoch 110, loss: 2.291916\n",
      "Epoch 111, loss: 2.292507\n",
      "Epoch 112, loss: 2.295214\n",
      "Epoch 113, loss: 2.291204\n",
      "Epoch 114, loss: 2.290113\n",
      "Epoch 115, loss: 2.291176\n",
      "Epoch 116, loss: 2.291489\n",
      "Epoch 117, loss: 2.293651\n",
      "Epoch 118, loss: 2.293969\n",
      "Epoch 119, loss: 2.292555\n",
      "Epoch 120, loss: 2.293507\n",
      "Epoch 121, loss: 2.293150\n",
      "Epoch 122, loss: 2.296091\n",
      "Epoch 123, loss: 2.290053\n",
      "Epoch 124, loss: 2.293959\n",
      "Epoch 125, loss: 2.292906\n",
      "Epoch 126, loss: 2.290447\n",
      "Epoch 127, loss: 2.294287\n",
      "Epoch 128, loss: 2.290031\n",
      "Epoch 129, loss: 2.286380\n",
      "Epoch 130, loss: 2.291509\n",
      "Epoch 131, loss: 2.286854\n",
      "Epoch 132, loss: 2.293281\n",
      "Epoch 133, loss: 2.290162\n",
      "Epoch 134, loss: 2.287926\n",
      "Epoch 135, loss: 2.290296\n",
      "Epoch 136, loss: 2.285410\n",
      "Epoch 137, loss: 2.292413\n",
      "Epoch 138, loss: 2.291825\n",
      "Epoch 139, loss: 2.292079\n",
      "Epoch 140, loss: 2.291167\n",
      "Epoch 141, loss: 2.294097\n",
      "Epoch 142, loss: 2.286647\n",
      "Epoch 143, loss: 2.286268\n",
      "Epoch 144, loss: 2.286404\n",
      "Epoch 145, loss: 2.289786\n",
      "Epoch 146, loss: 2.291244\n",
      "Epoch 147, loss: 2.291032\n",
      "Epoch 148, loss: 2.289080\n",
      "Epoch 149, loss: 2.289595\n",
      "Epoch 150, loss: 2.286344\n",
      "Epoch 151, loss: 2.290711\n",
      "Epoch 152, loss: 2.292019\n",
      "Epoch 153, loss: 2.286612\n",
      "Epoch 154, loss: 2.285711\n",
      "Epoch 155, loss: 2.285750\n",
      "Epoch 156, loss: 2.285990\n",
      "Epoch 157, loss: 2.288216\n",
      "Epoch 158, loss: 2.291134\n",
      "Epoch 159, loss: 2.287687\n",
      "Epoch 160, loss: 2.291807\n",
      "Epoch 161, loss: 2.288475\n",
      "Epoch 162, loss: 2.289184\n",
      "Epoch 163, loss: 2.284003\n",
      "Epoch 164, loss: 2.285082\n",
      "Epoch 165, loss: 2.285197\n",
      "Epoch 166, loss: 2.283582\n",
      "Epoch 167, loss: 2.287732\n",
      "Epoch 168, loss: 2.287149\n",
      "Epoch 169, loss: 2.285965\n",
      "Epoch 170, loss: 2.285636\n",
      "Epoch 171, loss: 2.290612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, loss: 2.294350\n",
      "Epoch 173, loss: 2.284442\n",
      "Epoch 174, loss: 2.289677\n",
      "Epoch 175, loss: 2.286897\n",
      "Epoch 176, loss: 2.291045\n",
      "Epoch 177, loss: 2.285322\n",
      "Epoch 178, loss: 2.286536\n",
      "Epoch 179, loss: 2.283758\n",
      "Epoch 180, loss: 2.284789\n",
      "Epoch 181, loss: 2.285522\n",
      "Epoch 182, loss: 2.282855\n",
      "Epoch 183, loss: 2.286171\n",
      "Epoch 184, loss: 2.288594\n",
      "Epoch 185, loss: 2.284811\n",
      "Epoch 186, loss: 2.288736\n",
      "Epoch 187, loss: 2.281424\n",
      "Epoch 188, loss: 2.288831\n",
      "Epoch 189, loss: 2.288126\n",
      "Epoch 190, loss: 2.289444\n",
      "Epoch 191, loss: 2.290647\n",
      "Epoch 192, loss: 2.286166\n",
      "Epoch 193, loss: 2.290042\n",
      "Epoch 194, loss: 2.283250\n",
      "Epoch 195, loss: 2.285556\n",
      "Epoch 196, loss: 2.282813\n",
      "Epoch 197, loss: 2.287253\n",
      "Epoch 198, loss: 2.283727\n",
      "Epoch 199, loss: 2.282269\n",
      "Epoch 0, loss: 2.302288\n",
      "Epoch 1, loss: 2.302398\n",
      "Epoch 2, loss: 2.302830\n",
      "Epoch 3, loss: 2.301716\n",
      "Epoch 4, loss: 2.302600\n",
      "Epoch 5, loss: 2.301255\n",
      "Epoch 6, loss: 2.302376\n",
      "Epoch 7, loss: 2.301897\n",
      "Epoch 8, loss: 2.301072\n",
      "Epoch 9, loss: 2.301088\n",
      "Epoch 10, loss: 2.302569\n",
      "Epoch 11, loss: 2.301949\n",
      "Epoch 12, loss: 2.302108\n",
      "Epoch 13, loss: 2.300081\n",
      "Epoch 14, loss: 2.300602\n",
      "Epoch 15, loss: 2.300807\n",
      "Epoch 16, loss: 2.300357\n",
      "Epoch 17, loss: 2.301087\n",
      "Epoch 18, loss: 2.300311\n",
      "Epoch 19, loss: 2.301404\n",
      "Epoch 20, loss: 2.301887\n",
      "Epoch 21, loss: 2.299827\n",
      "Epoch 22, loss: 2.300215\n",
      "Epoch 23, loss: 2.299775\n",
      "Epoch 24, loss: 2.299007\n",
      "Epoch 25, loss: 2.301529\n",
      "Epoch 26, loss: 2.299424\n",
      "Epoch 27, loss: 2.299337\n",
      "Epoch 28, loss: 2.299962\n",
      "Epoch 29, loss: 2.298607\n",
      "Epoch 30, loss: 2.300057\n",
      "Epoch 31, loss: 2.299910\n",
      "Epoch 32, loss: 2.300051\n",
      "Epoch 33, loss: 2.298729\n",
      "Epoch 34, loss: 2.298827\n",
      "Epoch 35, loss: 2.298128\n",
      "Epoch 36, loss: 2.300755\n",
      "Epoch 37, loss: 2.300229\n",
      "Epoch 38, loss: 2.298092\n",
      "Epoch 39, loss: 2.297410\n",
      "Epoch 40, loss: 2.299519\n",
      "Epoch 41, loss: 2.297185\n",
      "Epoch 42, loss: 2.298590\n",
      "Epoch 43, loss: 2.299422\n",
      "Epoch 44, loss: 2.296717\n",
      "Epoch 45, loss: 2.297986\n",
      "Epoch 46, loss: 2.297674\n",
      "Epoch 47, loss: 2.298376\n",
      "Epoch 48, loss: 2.297754\n",
      "Epoch 49, loss: 2.298516\n",
      "Epoch 50, loss: 2.297931\n",
      "Epoch 51, loss: 2.296080\n",
      "Epoch 52, loss: 2.297161\n",
      "Epoch 53, loss: 2.299661\n",
      "Epoch 54, loss: 2.299920\n",
      "Epoch 55, loss: 2.296319\n",
      "Epoch 56, loss: 2.297820\n",
      "Epoch 57, loss: 2.298386\n",
      "Epoch 58, loss: 2.296926\n",
      "Epoch 59, loss: 2.298242\n",
      "Epoch 60, loss: 2.297338\n",
      "Epoch 61, loss: 2.297279\n",
      "Epoch 62, loss: 2.297009\n",
      "Epoch 63, loss: 2.294555\n",
      "Epoch 64, loss: 2.296218\n",
      "Epoch 65, loss: 2.294854\n",
      "Epoch 66, loss: 2.298006\n",
      "Epoch 67, loss: 2.297590\n",
      "Epoch 68, loss: 2.293269\n",
      "Epoch 69, loss: 2.295933\n",
      "Epoch 70, loss: 2.294975\n",
      "Epoch 71, loss: 2.299507\n",
      "Epoch 72, loss: 2.293837\n",
      "Epoch 73, loss: 2.294997\n",
      "Epoch 74, loss: 2.294016\n",
      "Epoch 75, loss: 2.293627\n",
      "Epoch 76, loss: 2.294594\n",
      "Epoch 77, loss: 2.296019\n",
      "Epoch 78, loss: 2.297661\n",
      "Epoch 79, loss: 2.290911\n",
      "Epoch 80, loss: 2.292518\n",
      "Epoch 81, loss: 2.293120\n",
      "Epoch 82, loss: 2.298769\n",
      "Epoch 83, loss: 2.294389\n",
      "Epoch 84, loss: 2.296187\n",
      "Epoch 85, loss: 2.294313\n",
      "Epoch 86, loss: 2.294673\n",
      "Epoch 87, loss: 2.295789\n",
      "Epoch 88, loss: 2.296409\n",
      "Epoch 89, loss: 2.295036\n",
      "Epoch 90, loss: 2.292595\n",
      "Epoch 91, loss: 2.290982\n",
      "Epoch 92, loss: 2.295016\n",
      "Epoch 93, loss: 2.294817\n",
      "Epoch 94, loss: 2.293602\n",
      "Epoch 95, loss: 2.292752\n",
      "Epoch 96, loss: 2.295097\n",
      "Epoch 97, loss: 2.292225\n",
      "Epoch 98, loss: 2.292961\n",
      "Epoch 99, loss: 2.294494\n",
      "Epoch 100, loss: 2.293578\n",
      "Epoch 101, loss: 2.288670\n",
      "Epoch 102, loss: 2.293112\n",
      "Epoch 103, loss: 2.295124\n",
      "Epoch 104, loss: 2.292775\n",
      "Epoch 105, loss: 2.296565\n",
      "Epoch 106, loss: 2.297496\n",
      "Epoch 107, loss: 2.293179\n",
      "Epoch 108, loss: 2.289741\n",
      "Epoch 109, loss: 2.292155\n",
      "Epoch 110, loss: 2.295390\n",
      "Epoch 111, loss: 2.288669\n",
      "Epoch 112, loss: 2.295033\n",
      "Epoch 113, loss: 2.294975\n",
      "Epoch 114, loss: 2.292605\n",
      "Epoch 115, loss: 2.290271\n",
      "Epoch 116, loss: 2.293323\n",
      "Epoch 117, loss: 2.292718\n",
      "Epoch 118, loss: 2.291126\n",
      "Epoch 119, loss: 2.293302\n",
      "Epoch 120, loss: 2.292042\n",
      "Epoch 121, loss: 2.291777\n",
      "Epoch 122, loss: 2.289180\n",
      "Epoch 123, loss: 2.288894\n",
      "Epoch 124, loss: 2.289033\n",
      "Epoch 125, loss: 2.292036\n",
      "Epoch 126, loss: 2.291474\n",
      "Epoch 127, loss: 2.293530\n",
      "Epoch 128, loss: 2.293803\n",
      "Epoch 129, loss: 2.291728\n",
      "Epoch 130, loss: 2.291367\n",
      "Epoch 131, loss: 2.288755\n",
      "Epoch 132, loss: 2.286737\n",
      "Epoch 133, loss: 2.288866\n",
      "Epoch 134, loss: 2.290021\n",
      "Epoch 135, loss: 2.292237\n",
      "Epoch 136, loss: 2.287852\n",
      "Epoch 137, loss: 2.287279\n",
      "Epoch 138, loss: 2.292849\n",
      "Epoch 139, loss: 2.288306\n",
      "Epoch 140, loss: 2.290230\n",
      "Epoch 141, loss: 2.286803\n",
      "Epoch 142, loss: 2.290492\n",
      "Epoch 143, loss: 2.287906\n",
      "Epoch 144, loss: 2.290613\n",
      "Epoch 145, loss: 2.287060\n",
      "Epoch 146, loss: 2.291285\n",
      "Epoch 147, loss: 2.288847\n",
      "Epoch 148, loss: 2.292035\n",
      "Epoch 149, loss: 2.292235\n",
      "Epoch 150, loss: 2.287600\n",
      "Epoch 151, loss: 2.289521\n",
      "Epoch 152, loss: 2.285158\n",
      "Epoch 153, loss: 2.290359\n",
      "Epoch 154, loss: 2.284667\n",
      "Epoch 155, loss: 2.288322\n",
      "Epoch 156, loss: 2.287023\n",
      "Epoch 157, loss: 2.287783\n",
      "Epoch 158, loss: 2.285840\n",
      "Epoch 159, loss: 2.287358\n",
      "Epoch 160, loss: 2.288221\n",
      "Epoch 161, loss: 2.286749\n",
      "Epoch 162, loss: 2.290525\n",
      "Epoch 163, loss: 2.285438\n",
      "Epoch 164, loss: 2.284278\n",
      "Epoch 165, loss: 2.289354\n",
      "Epoch 166, loss: 2.282744\n",
      "Epoch 167, loss: 2.284062\n",
      "Epoch 168, loss: 2.287221\n",
      "Epoch 169, loss: 2.285630\n",
      "Epoch 170, loss: 2.288939\n",
      "Epoch 171, loss: 2.284413\n",
      "Epoch 172, loss: 2.290939\n",
      "Epoch 173, loss: 2.290072\n",
      "Epoch 174, loss: 2.287861\n",
      "Epoch 175, loss: 2.286137\n",
      "Epoch 176, loss: 2.285289\n",
      "Epoch 177, loss: 2.282837\n",
      "Epoch 178, loss: 2.287643\n",
      "Epoch 179, loss: 2.287205\n",
      "Epoch 180, loss: 2.286585\n",
      "Epoch 181, loss: 2.284355\n",
      "Epoch 182, loss: 2.285997\n",
      "Epoch 183, loss: 2.284813\n",
      "Epoch 184, loss: 2.284568\n",
      "Epoch 185, loss: 2.286670\n",
      "Epoch 186, loss: 2.288465\n",
      "Epoch 187, loss: 2.285029\n",
      "Epoch 188, loss: 2.285935\n",
      "Epoch 189, loss: 2.284825\n",
      "Epoch 190, loss: 2.287582\n",
      "Epoch 191, loss: 2.281119\n",
      "Epoch 192, loss: 2.289311\n",
      "Epoch 193, loss: 2.287394\n",
      "Epoch 194, loss: 2.289755\n",
      "Epoch 195, loss: 2.292634\n",
      "Epoch 196, loss: 2.283112\n",
      "Epoch 197, loss: 2.285921\n",
      "Epoch 198, loss: 2.286859\n",
      "Epoch 199, loss: 2.290864\n",
      "Epoch 0, loss: 2.303729\n",
      "Epoch 1, loss: 2.302667\n",
      "Epoch 2, loss: 2.302208\n",
      "Epoch 3, loss: 2.303002\n",
      "Epoch 4, loss: 2.303474\n",
      "Epoch 5, loss: 2.302721\n",
      "Epoch 6, loss: 2.301406\n",
      "Epoch 7, loss: 2.302683\n",
      "Epoch 8, loss: 2.303690\n",
      "Epoch 9, loss: 2.301392\n",
      "Epoch 10, loss: 2.302323\n",
      "Epoch 11, loss: 2.302532\n",
      "Epoch 12, loss: 2.302307\n",
      "Epoch 13, loss: 2.301938\n",
      "Epoch 14, loss: 2.301999\n",
      "Epoch 15, loss: 2.302100\n",
      "Epoch 16, loss: 2.303416\n",
      "Epoch 17, loss: 2.303004\n",
      "Epoch 18, loss: 2.302428\n",
      "Epoch 19, loss: 2.303775\n",
      "Epoch 20, loss: 2.302060\n",
      "Epoch 21, loss: 2.303148\n",
      "Epoch 22, loss: 2.302931\n",
      "Epoch 23, loss: 2.302131\n",
      "Epoch 24, loss: 2.301655\n",
      "Epoch 25, loss: 2.302592\n",
      "Epoch 26, loss: 2.301851\n",
      "Epoch 27, loss: 2.303148\n",
      "Epoch 28, loss: 2.303316\n",
      "Epoch 29, loss: 2.302055\n",
      "Epoch 30, loss: 2.301875\n",
      "Epoch 31, loss: 2.303253\n",
      "Epoch 32, loss: 2.303344\n",
      "Epoch 33, loss: 2.302432\n",
      "Epoch 34, loss: 2.303332\n",
      "Epoch 35, loss: 2.301862\n",
      "Epoch 36, loss: 2.302141\n",
      "Epoch 37, loss: 2.302977\n",
      "Epoch 38, loss: 2.302409\n",
      "Epoch 39, loss: 2.302262\n",
      "Epoch 40, loss: 2.302778\n",
      "Epoch 41, loss: 2.302032\n",
      "Epoch 42, loss: 2.302640\n",
      "Epoch 43, loss: 2.302355\n",
      "Epoch 44, loss: 2.303249\n",
      "Epoch 45, loss: 2.302563\n",
      "Epoch 46, loss: 2.302439\n",
      "Epoch 47, loss: 2.303792\n",
      "Epoch 48, loss: 2.301714\n",
      "Epoch 49, loss: 2.303406\n",
      "Epoch 50, loss: 2.302772\n",
      "Epoch 51, loss: 2.302285\n",
      "Epoch 52, loss: 2.302025\n",
      "Epoch 53, loss: 2.302153\n",
      "Epoch 54, loss: 2.301820\n",
      "Epoch 55, loss: 2.301578\n",
      "Epoch 56, loss: 2.302178\n",
      "Epoch 57, loss: 2.302286\n",
      "Epoch 58, loss: 2.302383\n",
      "Epoch 59, loss: 2.302310\n",
      "Epoch 60, loss: 2.301632\n",
      "Epoch 61, loss: 2.302290\n",
      "Epoch 62, loss: 2.302488\n",
      "Epoch 63, loss: 2.302926\n",
      "Epoch 64, loss: 2.302917\n",
      "Epoch 65, loss: 2.302859\n",
      "Epoch 66, loss: 2.301695\n",
      "Epoch 67, loss: 2.302690\n",
      "Epoch 68, loss: 2.300822\n",
      "Epoch 69, loss: 2.301900\n",
      "Epoch 70, loss: 2.300644\n",
      "Epoch 71, loss: 2.301043\n",
      "Epoch 72, loss: 2.301672\n",
      "Epoch 73, loss: 2.302193\n",
      "Epoch 74, loss: 2.301549\n",
      "Epoch 75, loss: 2.302133\n",
      "Epoch 76, loss: 2.303448\n",
      "Epoch 77, loss: 2.301378\n",
      "Epoch 78, loss: 2.302268\n",
      "Epoch 79, loss: 2.301612\n",
      "Epoch 80, loss: 2.301537\n",
      "Epoch 81, loss: 2.300789\n",
      "Epoch 82, loss: 2.302080\n",
      "Epoch 83, loss: 2.301767\n",
      "Epoch 84, loss: 2.302095\n",
      "Epoch 85, loss: 2.302431\n",
      "Epoch 86, loss: 2.301992\n",
      "Epoch 87, loss: 2.301868\n",
      "Epoch 88, loss: 2.300849\n",
      "Epoch 89, loss: 2.301557\n",
      "Epoch 90, loss: 2.302144\n",
      "Epoch 91, loss: 2.301712\n",
      "Epoch 92, loss: 2.302329\n",
      "Epoch 93, loss: 2.301574\n",
      "Epoch 94, loss: 2.301737\n",
      "Epoch 95, loss: 2.300863\n",
      "Epoch 96, loss: 2.301114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, loss: 2.301647\n",
      "Epoch 98, loss: 2.302184\n",
      "Epoch 99, loss: 2.301509\n",
      "Epoch 100, loss: 2.301243\n",
      "Epoch 101, loss: 2.301187\n",
      "Epoch 102, loss: 2.301478\n",
      "Epoch 103, loss: 2.302114\n",
      "Epoch 104, loss: 2.302338\n",
      "Epoch 105, loss: 2.300661\n",
      "Epoch 106, loss: 2.300597\n",
      "Epoch 107, loss: 2.301840\n",
      "Epoch 108, loss: 2.301675\n",
      "Epoch 109, loss: 2.301221\n",
      "Epoch 110, loss: 2.300464\n",
      "Epoch 111, loss: 2.301324\n",
      "Epoch 112, loss: 2.300783\n",
      "Epoch 113, loss: 2.301921\n",
      "Epoch 114, loss: 2.301991\n",
      "Epoch 115, loss: 2.302816\n",
      "Epoch 116, loss: 2.301424\n",
      "Epoch 117, loss: 2.301771\n",
      "Epoch 118, loss: 2.300661\n",
      "Epoch 119, loss: 2.303141\n",
      "Epoch 120, loss: 2.301493\n",
      "Epoch 121, loss: 2.301709\n",
      "Epoch 122, loss: 2.302727\n",
      "Epoch 123, loss: 2.301785\n",
      "Epoch 124, loss: 2.301192\n",
      "Epoch 125, loss: 2.300859\n",
      "Epoch 126, loss: 2.300600\n",
      "Epoch 127, loss: 2.301239\n",
      "Epoch 128, loss: 2.301910\n",
      "Epoch 129, loss: 2.301644\n",
      "Epoch 130, loss: 2.301560\n",
      "Epoch 131, loss: 2.300246\n",
      "Epoch 132, loss: 2.300852\n",
      "Epoch 133, loss: 2.301320\n",
      "Epoch 134, loss: 2.301561\n",
      "Epoch 135, loss: 2.301040\n",
      "Epoch 136, loss: 2.302040\n",
      "Epoch 137, loss: 2.301702\n",
      "Epoch 138, loss: 2.301183\n",
      "Epoch 139, loss: 2.302406\n",
      "Epoch 140, loss: 2.300797\n",
      "Epoch 141, loss: 2.301519\n",
      "Epoch 142, loss: 2.301547\n",
      "Epoch 143, loss: 2.301408\n",
      "Epoch 144, loss: 2.301368\n",
      "Epoch 145, loss: 2.300826\n",
      "Epoch 146, loss: 2.302294\n",
      "Epoch 147, loss: 2.301909\n",
      "Epoch 148, loss: 2.301254\n",
      "Epoch 149, loss: 2.301133\n",
      "Epoch 150, loss: 2.301620\n",
      "Epoch 151, loss: 2.300920\n",
      "Epoch 152, loss: 2.300556\n",
      "Epoch 153, loss: 2.300617\n",
      "Epoch 154, loss: 2.301232\n",
      "Epoch 155, loss: 2.299525\n",
      "Epoch 156, loss: 2.300479\n",
      "Epoch 157, loss: 2.300612\n",
      "Epoch 158, loss: 2.301861\n",
      "Epoch 159, loss: 2.299906\n",
      "Epoch 160, loss: 2.301172\n",
      "Epoch 161, loss: 2.301104\n",
      "Epoch 162, loss: 2.301234\n",
      "Epoch 163, loss: 2.301604\n",
      "Epoch 164, loss: 2.300418\n",
      "Epoch 165, loss: 2.301332\n",
      "Epoch 166, loss: 2.300271\n",
      "Epoch 167, loss: 2.300720\n",
      "Epoch 168, loss: 2.301063\n",
      "Epoch 169, loss: 2.300664\n",
      "Epoch 170, loss: 2.300463\n",
      "Epoch 171, loss: 2.300629\n",
      "Epoch 172, loss: 2.301239\n",
      "Epoch 173, loss: 2.300753\n",
      "Epoch 174, loss: 2.300728\n",
      "Epoch 175, loss: 2.301010\n",
      "Epoch 176, loss: 2.300802\n",
      "Epoch 177, loss: 2.300133\n",
      "Epoch 178, loss: 2.300464\n",
      "Epoch 179, loss: 2.301299\n",
      "Epoch 180, loss: 2.301192\n",
      "Epoch 181, loss: 2.300351\n",
      "Epoch 182, loss: 2.300023\n",
      "Epoch 183, loss: 2.301145\n",
      "Epoch 184, loss: 2.302365\n",
      "Epoch 185, loss: 2.301130\n",
      "Epoch 186, loss: 2.300036\n",
      "Epoch 187, loss: 2.300357\n",
      "Epoch 188, loss: 2.300445\n",
      "Epoch 189, loss: 2.300244\n",
      "Epoch 190, loss: 2.301195\n",
      "Epoch 191, loss: 2.300528\n",
      "Epoch 192, loss: 2.301876\n",
      "Epoch 193, loss: 2.300424\n",
      "Epoch 194, loss: 2.300972\n",
      "Epoch 195, loss: 2.300736\n",
      "Epoch 196, loss: 2.300307\n",
      "Epoch 197, loss: 2.300040\n",
      "Epoch 198, loss: 2.300093\n",
      "Epoch 199, loss: 2.300837\n",
      "Epoch 0, loss: 2.303001\n",
      "Epoch 1, loss: 2.302783\n",
      "Epoch 2, loss: 2.302995\n",
      "Epoch 3, loss: 2.302415\n",
      "Epoch 4, loss: 2.302644\n",
      "Epoch 5, loss: 2.302432\n",
      "Epoch 6, loss: 2.302187\n",
      "Epoch 7, loss: 2.303693\n",
      "Epoch 8, loss: 2.302660\n",
      "Epoch 9, loss: 2.302283\n",
      "Epoch 10, loss: 2.303138\n",
      "Epoch 11, loss: 2.303199\n",
      "Epoch 12, loss: 2.302686\n",
      "Epoch 13, loss: 2.302411\n",
      "Epoch 14, loss: 2.303201\n",
      "Epoch 15, loss: 2.302173\n",
      "Epoch 16, loss: 2.302448\n",
      "Epoch 17, loss: 2.302659\n",
      "Epoch 18, loss: 2.302262\n",
      "Epoch 19, loss: 2.301787\n",
      "Epoch 20, loss: 2.303079\n",
      "Epoch 21, loss: 2.302809\n",
      "Epoch 22, loss: 2.301719\n",
      "Epoch 23, loss: 2.302518\n",
      "Epoch 24, loss: 2.302559\n",
      "Epoch 25, loss: 2.302059\n",
      "Epoch 26, loss: 2.301596\n",
      "Epoch 27, loss: 2.301690\n",
      "Epoch 28, loss: 2.301648\n",
      "Epoch 29, loss: 2.301972\n",
      "Epoch 30, loss: 2.301133\n",
      "Epoch 31, loss: 2.302384\n",
      "Epoch 32, loss: 2.302584\n",
      "Epoch 33, loss: 2.301767\n",
      "Epoch 34, loss: 2.302912\n",
      "Epoch 35, loss: 2.302028\n",
      "Epoch 36, loss: 2.302197\n",
      "Epoch 37, loss: 2.302751\n",
      "Epoch 38, loss: 2.302463\n",
      "Epoch 39, loss: 2.301618\n",
      "Epoch 40, loss: 2.301852\n",
      "Epoch 41, loss: 2.302133\n",
      "Epoch 42, loss: 2.302284\n",
      "Epoch 43, loss: 2.302592\n",
      "Epoch 44, loss: 2.303217\n",
      "Epoch 45, loss: 2.302283\n",
      "Epoch 46, loss: 2.301857\n",
      "Epoch 47, loss: 2.302874\n",
      "Epoch 48, loss: 2.301632\n",
      "Epoch 49, loss: 2.303397\n",
      "Epoch 50, loss: 2.300587\n",
      "Epoch 51, loss: 2.302157\n",
      "Epoch 52, loss: 2.301612\n",
      "Epoch 53, loss: 2.302089\n",
      "Epoch 54, loss: 2.302144\n",
      "Epoch 55, loss: 2.301538\n",
      "Epoch 56, loss: 2.302339\n",
      "Epoch 57, loss: 2.303054\n",
      "Epoch 58, loss: 2.302749\n",
      "Epoch 59, loss: 2.301729\n",
      "Epoch 60, loss: 2.302919\n",
      "Epoch 61, loss: 2.301508\n",
      "Epoch 62, loss: 2.302100\n",
      "Epoch 63, loss: 2.302004\n",
      "Epoch 64, loss: 2.302419\n",
      "Epoch 65, loss: 2.301948\n",
      "Epoch 66, loss: 2.302904\n",
      "Epoch 67, loss: 2.301746\n",
      "Epoch 68, loss: 2.301626\n",
      "Epoch 69, loss: 2.301394\n",
      "Epoch 70, loss: 2.302201\n",
      "Epoch 71, loss: 2.302118\n",
      "Epoch 72, loss: 2.303352\n",
      "Epoch 73, loss: 2.302108\n",
      "Epoch 74, loss: 2.301753\n",
      "Epoch 75, loss: 2.302052\n",
      "Epoch 76, loss: 2.301869\n",
      "Epoch 77, loss: 2.301912\n",
      "Epoch 78, loss: 2.302228\n",
      "Epoch 79, loss: 2.301525\n",
      "Epoch 80, loss: 2.301283\n",
      "Epoch 81, loss: 2.301489\n",
      "Epoch 82, loss: 2.301955\n",
      "Epoch 83, loss: 2.301309\n",
      "Epoch 84, loss: 2.302403\n",
      "Epoch 85, loss: 2.301442\n",
      "Epoch 86, loss: 2.301894\n",
      "Epoch 87, loss: 2.300839\n",
      "Epoch 88, loss: 2.300899\n",
      "Epoch 89, loss: 2.301508\n",
      "Epoch 90, loss: 2.300936\n",
      "Epoch 91, loss: 2.302266\n",
      "Epoch 92, loss: 2.301888\n",
      "Epoch 93, loss: 2.301532\n",
      "Epoch 94, loss: 2.302166\n",
      "Epoch 95, loss: 2.301360\n",
      "Epoch 96, loss: 2.301903\n",
      "Epoch 97, loss: 2.300730\n",
      "Epoch 98, loss: 2.300947\n",
      "Epoch 99, loss: 2.301120\n",
      "Epoch 100, loss: 2.301449\n",
      "Epoch 101, loss: 2.302082\n",
      "Epoch 102, loss: 2.301157\n",
      "Epoch 103, loss: 2.301468\n",
      "Epoch 104, loss: 2.302463\n",
      "Epoch 105, loss: 2.302240\n",
      "Epoch 106, loss: 2.301315\n",
      "Epoch 107, loss: 2.301082\n",
      "Epoch 108, loss: 2.301657\n",
      "Epoch 109, loss: 2.301541\n",
      "Epoch 110, loss: 2.300606\n",
      "Epoch 111, loss: 2.300885\n",
      "Epoch 112, loss: 2.302594\n",
      "Epoch 113, loss: 2.301014\n",
      "Epoch 114, loss: 2.301503\n",
      "Epoch 115, loss: 2.301493\n",
      "Epoch 116, loss: 2.301765\n",
      "Epoch 117, loss: 2.301800\n",
      "Epoch 118, loss: 2.301330\n",
      "Epoch 119, loss: 2.301729\n",
      "Epoch 120, loss: 2.302160\n",
      "Epoch 121, loss: 2.301564\n",
      "Epoch 122, loss: 2.301848\n",
      "Epoch 123, loss: 2.300695\n",
      "Epoch 124, loss: 2.302555\n",
      "Epoch 125, loss: 2.301703\n",
      "Epoch 126, loss: 2.300498\n",
      "Epoch 127, loss: 2.300137\n",
      "Epoch 128, loss: 2.301387\n",
      "Epoch 129, loss: 2.300821\n",
      "Epoch 130, loss: 2.301771\n",
      "Epoch 131, loss: 2.301659\n",
      "Epoch 132, loss: 2.301173\n",
      "Epoch 133, loss: 2.301165\n",
      "Epoch 134, loss: 2.301328\n",
      "Epoch 135, loss: 2.301176\n",
      "Epoch 136, loss: 2.301566\n",
      "Epoch 137, loss: 2.300437\n",
      "Epoch 138, loss: 2.301740\n",
      "Epoch 139, loss: 2.300589\n",
      "Epoch 140, loss: 2.301166\n",
      "Epoch 141, loss: 2.301201\n",
      "Epoch 142, loss: 2.298974\n",
      "Epoch 143, loss: 2.301216\n",
      "Epoch 144, loss: 2.299179\n",
      "Epoch 145, loss: 2.300784\n",
      "Epoch 146, loss: 2.301733\n",
      "Epoch 147, loss: 2.302049\n",
      "Epoch 148, loss: 2.300041\n",
      "Epoch 149, loss: 2.299884\n",
      "Epoch 150, loss: 2.301712\n",
      "Epoch 151, loss: 2.299468\n",
      "Epoch 152, loss: 2.301331\n",
      "Epoch 153, loss: 2.300936\n",
      "Epoch 154, loss: 2.300740\n",
      "Epoch 155, loss: 2.300770\n",
      "Epoch 156, loss: 2.301688\n",
      "Epoch 157, loss: 2.299724\n",
      "Epoch 158, loss: 2.301723\n",
      "Epoch 159, loss: 2.300383\n",
      "Epoch 160, loss: 2.300802\n",
      "Epoch 161, loss: 2.301631\n",
      "Epoch 162, loss: 2.299033\n",
      "Epoch 163, loss: 2.301570\n",
      "Epoch 164, loss: 2.300978\n",
      "Epoch 165, loss: 2.301531\n",
      "Epoch 166, loss: 2.300588\n",
      "Epoch 167, loss: 2.300210\n",
      "Epoch 168, loss: 2.301070\n",
      "Epoch 169, loss: 2.301342\n",
      "Epoch 170, loss: 2.299953\n",
      "Epoch 171, loss: 2.299457\n",
      "Epoch 172, loss: 2.300349\n",
      "Epoch 173, loss: 2.299412\n",
      "Epoch 174, loss: 2.300536\n",
      "Epoch 175, loss: 2.300162\n",
      "Epoch 176, loss: 2.300718\n",
      "Epoch 177, loss: 2.300105\n",
      "Epoch 178, loss: 2.300661\n",
      "Epoch 179, loss: 2.300830\n",
      "Epoch 180, loss: 2.300240\n",
      "Epoch 181, loss: 2.301938\n",
      "Epoch 182, loss: 2.300845\n",
      "Epoch 183, loss: 2.301595\n",
      "Epoch 184, loss: 2.300890\n",
      "Epoch 185, loss: 2.299956\n",
      "Epoch 186, loss: 2.300562\n",
      "Epoch 187, loss: 2.300702\n",
      "Epoch 188, loss: 2.299428\n",
      "Epoch 189, loss: 2.301088\n",
      "Epoch 190, loss: 2.300054\n",
      "Epoch 191, loss: 2.300285\n",
      "Epoch 192, loss: 2.300821\n",
      "Epoch 193, loss: 2.301932\n",
      "Epoch 194, loss: 2.301003\n",
      "Epoch 195, loss: 2.300511\n",
      "Epoch 196, loss: 2.301741\n",
      "Epoch 197, loss: 2.301196\n",
      "Epoch 198, loss: 2.300619\n",
      "Epoch 199, loss: 2.299023\n",
      "Epoch 0, loss: 2.301832\n",
      "Epoch 1, loss: 2.302655\n",
      "Epoch 2, loss: 2.303456\n",
      "Epoch 3, loss: 2.302633\n",
      "Epoch 4, loss: 2.303109\n",
      "Epoch 5, loss: 2.302721\n",
      "Epoch 6, loss: 2.302136\n",
      "Epoch 7, loss: 2.302735\n",
      "Epoch 8, loss: 2.302424\n",
      "Epoch 9, loss: 2.301803\n",
      "Epoch 10, loss: 2.303022\n",
      "Epoch 11, loss: 2.302691\n",
      "Epoch 12, loss: 2.302744\n",
      "Epoch 13, loss: 2.302334\n",
      "Epoch 14, loss: 2.302108\n",
      "Epoch 15, loss: 2.302843\n",
      "Epoch 16, loss: 2.302458\n",
      "Epoch 17, loss: 2.301665\n",
      "Epoch 18, loss: 2.303706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, loss: 2.301641\n",
      "Epoch 20, loss: 2.303168\n",
      "Epoch 21, loss: 2.302908\n",
      "Epoch 22, loss: 2.303191\n",
      "Epoch 23, loss: 2.302904\n",
      "Epoch 24, loss: 2.303844\n",
      "Epoch 25, loss: 2.302029\n",
      "Epoch 26, loss: 2.302924\n",
      "Epoch 27, loss: 2.303878\n",
      "Epoch 28, loss: 2.302186\n",
      "Epoch 29, loss: 2.302904\n",
      "Epoch 30, loss: 2.302250\n",
      "Epoch 31, loss: 2.301264\n",
      "Epoch 32, loss: 2.302402\n",
      "Epoch 33, loss: 2.302085\n",
      "Epoch 34, loss: 2.302960\n",
      "Epoch 35, loss: 2.302274\n",
      "Epoch 36, loss: 2.302735\n",
      "Epoch 37, loss: 2.302862\n",
      "Epoch 38, loss: 2.302045\n",
      "Epoch 39, loss: 2.302697\n",
      "Epoch 40, loss: 2.303019\n",
      "Epoch 41, loss: 2.301694\n",
      "Epoch 42, loss: 2.303447\n",
      "Epoch 43, loss: 2.302126\n",
      "Epoch 44, loss: 2.302884\n",
      "Epoch 45, loss: 2.302660\n",
      "Epoch 46, loss: 2.302941\n",
      "Epoch 47, loss: 2.301766\n",
      "Epoch 48, loss: 2.301485\n",
      "Epoch 49, loss: 2.302113\n",
      "Epoch 50, loss: 2.302927\n",
      "Epoch 51, loss: 2.301555\n",
      "Epoch 52, loss: 2.302221\n",
      "Epoch 53, loss: 2.302080\n",
      "Epoch 54, loss: 2.302309\n",
      "Epoch 55, loss: 2.302961\n",
      "Epoch 56, loss: 2.301088\n",
      "Epoch 57, loss: 2.301894\n",
      "Epoch 58, loss: 2.302020\n",
      "Epoch 59, loss: 2.302146\n",
      "Epoch 60, loss: 2.301967\n",
      "Epoch 61, loss: 2.301902\n",
      "Epoch 62, loss: 2.302992\n",
      "Epoch 63, loss: 2.301404\n",
      "Epoch 64, loss: 2.301486\n",
      "Epoch 65, loss: 2.304439\n",
      "Epoch 66, loss: 2.302844\n",
      "Epoch 67, loss: 2.302195\n",
      "Epoch 68, loss: 2.301563\n",
      "Epoch 69, loss: 2.303170\n",
      "Epoch 70, loss: 2.300820\n",
      "Epoch 71, loss: 2.303090\n",
      "Epoch 72, loss: 2.302824\n",
      "Epoch 73, loss: 2.301678\n",
      "Epoch 74, loss: 2.302816\n",
      "Epoch 75, loss: 2.301637\n",
      "Epoch 76, loss: 2.301411\n",
      "Epoch 77, loss: 2.301665\n",
      "Epoch 78, loss: 2.301339\n",
      "Epoch 79, loss: 2.302362\n",
      "Epoch 80, loss: 2.302086\n",
      "Epoch 81, loss: 2.302108\n",
      "Epoch 82, loss: 2.302449\n",
      "Epoch 83, loss: 2.302039\n",
      "Epoch 84, loss: 2.302051\n",
      "Epoch 85, loss: 2.301492\n",
      "Epoch 86, loss: 2.302080\n",
      "Epoch 87, loss: 2.302324\n",
      "Epoch 88, loss: 2.300730\n",
      "Epoch 89, loss: 2.301317\n",
      "Epoch 90, loss: 2.301910\n",
      "Epoch 91, loss: 2.302128\n",
      "Epoch 92, loss: 2.302153\n",
      "Epoch 93, loss: 2.300783\n",
      "Epoch 94, loss: 2.301898\n",
      "Epoch 95, loss: 2.300675\n",
      "Epoch 96, loss: 2.301351\n",
      "Epoch 97, loss: 2.301360\n",
      "Epoch 98, loss: 2.302877\n",
      "Epoch 99, loss: 2.302448\n",
      "Epoch 100, loss: 2.301678\n",
      "Epoch 101, loss: 2.301231\n",
      "Epoch 102, loss: 2.303249\n",
      "Epoch 103, loss: 2.302415\n",
      "Epoch 104, loss: 2.302204\n",
      "Epoch 105, loss: 2.301903\n",
      "Epoch 106, loss: 2.301153\n",
      "Epoch 107, loss: 2.302189\n",
      "Epoch 108, loss: 2.301274\n",
      "Epoch 109, loss: 2.301658\n",
      "Epoch 110, loss: 2.303219\n",
      "Epoch 111, loss: 2.300553\n",
      "Epoch 112, loss: 2.301560\n",
      "Epoch 113, loss: 2.300464\n",
      "Epoch 114, loss: 2.301722\n",
      "Epoch 115, loss: 2.303154\n",
      "Epoch 116, loss: 2.302216\n",
      "Epoch 117, loss: 2.300758\n",
      "Epoch 118, loss: 2.302223\n",
      "Epoch 119, loss: 2.301728\n",
      "Epoch 120, loss: 2.302091\n",
      "Epoch 121, loss: 2.300137\n",
      "Epoch 122, loss: 2.301182\n",
      "Epoch 123, loss: 2.301801\n",
      "Epoch 124, loss: 2.301229\n",
      "Epoch 125, loss: 2.301281\n",
      "Epoch 126, loss: 2.301962\n",
      "Epoch 127, loss: 2.301582\n",
      "Epoch 128, loss: 2.301907\n",
      "Epoch 129, loss: 2.301763\n",
      "Epoch 130, loss: 2.302200\n",
      "Epoch 131, loss: 2.301146\n",
      "Epoch 132, loss: 2.301591\n",
      "Epoch 133, loss: 2.300987\n",
      "Epoch 134, loss: 2.301872\n",
      "Epoch 135, loss: 2.301058\n",
      "Epoch 136, loss: 2.300762\n",
      "Epoch 137, loss: 2.301428\n",
      "Epoch 138, loss: 2.301240\n",
      "Epoch 139, loss: 2.301120\n",
      "Epoch 140, loss: 2.301101\n",
      "Epoch 141, loss: 2.299742\n",
      "Epoch 142, loss: 2.300545\n",
      "Epoch 143, loss: 2.301531\n",
      "Epoch 144, loss: 2.301640\n",
      "Epoch 145, loss: 2.301259\n",
      "Epoch 146, loss: 2.300959\n",
      "Epoch 147, loss: 2.300773\n",
      "Epoch 148, loss: 2.301712\n",
      "Epoch 149, loss: 2.303005\n",
      "Epoch 150, loss: 2.300057\n",
      "Epoch 151, loss: 2.300533\n",
      "Epoch 152, loss: 2.301991\n",
      "Epoch 153, loss: 2.302009\n",
      "Epoch 154, loss: 2.301536\n",
      "Epoch 155, loss: 2.301135\n",
      "Epoch 156, loss: 2.300286\n",
      "Epoch 157, loss: 2.301319\n",
      "Epoch 158, loss: 2.300830\n",
      "Epoch 159, loss: 2.301112\n",
      "Epoch 160, loss: 2.299776\n",
      "Epoch 161, loss: 2.301701\n",
      "Epoch 162, loss: 2.302554\n",
      "Epoch 163, loss: 2.302004\n",
      "Epoch 164, loss: 2.301017\n",
      "Epoch 165, loss: 2.302088\n",
      "Epoch 166, loss: 2.301332\n",
      "Epoch 167, loss: 2.301451\n",
      "Epoch 168, loss: 2.301040\n",
      "Epoch 169, loss: 2.301413\n",
      "Epoch 170, loss: 2.300908\n",
      "Epoch 171, loss: 2.301643\n",
      "Epoch 172, loss: 2.300587\n",
      "Epoch 173, loss: 2.300750\n",
      "Epoch 174, loss: 2.302196\n",
      "Epoch 175, loss: 2.301481\n",
      "Epoch 176, loss: 2.302293\n",
      "Epoch 177, loss: 2.300798\n",
      "Epoch 178, loss: 2.302439\n",
      "Epoch 179, loss: 2.301320\n",
      "Epoch 180, loss: 2.300993\n",
      "Epoch 181, loss: 2.300191\n",
      "Epoch 182, loss: 2.300569\n",
      "Epoch 183, loss: 2.299448\n",
      "Epoch 184, loss: 2.300958\n",
      "Epoch 185, loss: 2.302661\n",
      "Epoch 186, loss: 2.300571\n",
      "Epoch 187, loss: 2.301245\n",
      "Epoch 188, loss: 2.301105\n",
      "Epoch 189, loss: 2.301412\n",
      "Epoch 190, loss: 2.300643\n",
      "Epoch 191, loss: 2.300725\n",
      "Epoch 192, loss: 2.300311\n",
      "Epoch 193, loss: 2.301837\n",
      "Epoch 194, loss: 2.302016\n",
      "Epoch 195, loss: 2.301135\n",
      "Epoch 196, loss: 2.301466\n",
      "Epoch 197, loss: 2.300605\n",
      "Epoch 198, loss: 2.299866\n",
      "Epoch 199, loss: 2.300500\n",
      "best validation accuracy achieved: 0.229000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5] \n",
    "reg_strengths = [1e-4, 1e-5, 1e-6] \n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, batch_size, lr, rs, num_epochs)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "            \n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.192000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
